{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP6pzzl84aTjQN0Uvi8tx7a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mithunareddy/NLP/blob/main/Lab_assignment_5_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "data= pd.read_csv('arxiv_data.csv.zip', engine='python', nrows=1000)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "H_NQ-irs1_eZ",
        "outputId": "55d25442-f75d-4c6e-c682-d2f44862bde3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              titles  \\\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4  Background-Foreground Segmentation for Interio...   \n",
              "\n",
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                         terms  \n",
              "0           ['cs.CV', 'cs.LG']  \n",
              "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
              "2           ['cs.CV', 'cs.AI']  \n",
              "3                    ['cs.CV']  \n",
              "4           ['cs.CV', 'cs.LG']  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ae4eed59-f0c8-446d-9036-1c1cd5e0d399\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae4eed59-f0c8-446d-9036-1c1cd5e0d399')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ae4eed59-f0c8-446d-9036-1c1cd5e0d399 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ae4eed59-f0c8-446d-9036-1c1cd5e0d399');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module\",\n          \"Studying the Plasticity in Deep Convolutional Neural Networks using Random Pruning\",\n          \"A Gentle Introduction to Deep Learning in Medical Image Processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"LIDAR point clouds and RGB-images are both extremely essential for 3D object\\ndetection. So many state-of-the-art 3D detection algorithms dedicate in fusing\\nthese two types of data effectively. However, their fusion methods based on\\nBirds Eye View (BEV) or voxel format are not accurate. In this paper, we\\npropose a novel fusion approach named Point-based Attentive Cont-conv\\nFusion(PACF) module, which fuses multi-sensor features directly on 3D points.\\nExcept for continuous convolution, we additionally add a Point-Pooling and an\\nAttentive Aggregation to make the fused features more expressive. Moreover,\\nbased on the PACF module, we propose a 3D multi-sensor multi-task network\\ncalled Pointcloud-Image RCNN(PI-RCNN as brief), which handles the image\\nsegmentation and 3D object detection tasks. PI-RCNN employs a segmentation\\nsub-network to extract full-resolution semantic feature maps from images and\\nthen fuses the multi-sensor features via powerful PACF module. Beneficial from\\nthe effectiveness of the PACF module and the expressive semantic features from\\nthe segmentation module, PI-RCNN can improve much in 3D object detection. We\\ndemonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3D\\nDetection benchmark, and our method can achieve state-of-the-art on the metric\\nof 3D AP.\",\n          \"Recently there has been a lot of work on pruning filters from deep\\nconvolutional neural networks (CNNs) with the intention of reducing\\ncomputations.The key idea is to rank the filters based on a certain criterion\\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\\nfilters are pruned away the remainder of the network is fine tuned and is shown\\nto give performance comparable to the original unpruned network. In this work,\\nwe report experiments which suggest that the comparable performance of the\\npruned network is not due to the specific criterion chosen but due to the\\ninherent plasticity of deep neural networks which allows them to recover from\\nthe loss of pruned filters once the rest of the filters are fine-tuned.\\nSpecifically we show counter-intuitive results wherein by randomly pruning\\n25-50% filters from deep CNNs we are able to obtain the same performance as\\nobtained by using state-of-the-art pruning methods. We empirically validate our\\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\\nneeds to be tested on only a small set of classes at test time (say, only\\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\\nclass specific pruning and show that even here a random pruning strategy gives\\nclose to state-of-the-art performance. Unlike existing approaches which mainly\\nfocus on the task of image classification, in this work we also report results\\non object detection and image segmentation. We show that using a simple random\\npruning strategy we can achieve significant speed up in object detection (74%\\nimprovement in fps) while retaining the same accuracy as that of the original\\nFaster RCNN model. Similarly we show that the performance of a pruned\\nSegmentation Network (SegNet) is actually very similar to that of the original\\nunpruned SegNet.\",\n          \"This paper tries to give a gentle introduction to deep learning in medical\\nimage processing, proceeding from theoretical foundations to applications. We\\nfirst discuss general reasons for the popularity of deep learning, including\\nseveral major breakthroughs in computer science. Next, we start reviewing the\\nfundamental basics of the perceptron and neural networks, along with some\\nfundamental theory that is often omitted. Doing so allows us to understand the\\nreasons for the rise of deep learning in many application domains. Obviously\\nmedical image processing is one of these areas which has been largely affected\\nby this rapid progress, in particular in image detection and recognition, image\\nsegmentation, image registration, and computer-aided diagnosis. There are also\\nrecent trends in physical simulation, modelling, and reconstruction that have\\nled to astonishing results. Yet, some of these approaches neglect prior\\nknowledge and hence bear the risk of producing implausible results. These\\napparent weaknesses highlight current limitations of deep learning. However, we\\nalso briefly discuss promising approaches that might be able to resolve these\\nproblems in the future.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 133,\n        \"samples\": [\n          \"['cs.CV', 'eess.IV', 'stat.AP', '62P99']\",\n          \"['cs.CV', 'cs.LG', 'I.2.1, I.4.6,']\",\n          \"['cs.CV', 'cs.CR', 'cs.LG']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\"\n",
        "        \"\\U0001F300-\\U0001F5FF\"\n",
        "        \"\\U0001F680-\\U0001F6FF\"\n",
        "        \"\\U0001F1E0-\\U0001F1FF\"\n",
        "        \"\\U00002702-\\U000027B0\"\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"\\U0001f926-\\U0001f937\"\n",
        "        \"\\U00010000-\\U0010ffff\"\n",
        "        \"\\u2640-\\u2642\"\n",
        "        \"\\u2600-\\u2B55\"\n",
        "        \"\\u200d\"\n",
        "        \"\\u23cf\"\n",
        "        \"\\u23e9\"\n",
        "        \"\\u231a\"\n",
        "        \"\\ufe0f\"\n",
        "        \"\\u3030\"\n",
        "        \"]+\", re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ],
      "metadata": {
        "id": "EvBug7FV2JZG"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['processed_summaries'] = data['summaries'].apply(preprocess_text)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "qISVEw8F2NHT",
        "outputId": "af2df630-43ae-48e9-d537-ed9a498ad315"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              titles  \\\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4  Background-Foreground Segmentation for Interio...   \n",
              "\n",
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                         terms  \\\n",
              "0           ['cs.CV', 'cs.LG']   \n",
              "1  ['cs.CV', 'cs.AI', 'cs.LG']   \n",
              "2           ['cs.CV', 'cs.AI']   \n",
              "3                    ['cs.CV']   \n",
              "4           ['cs.CV', 'cs.LG']   \n",
              "\n",
              "                                 processed_summaries  \n",
              "0  stereo matching is one of the widely used tech...  \n",
              "1  the recent advancements in artificial intellig...  \n",
              "2  in this paper we proposed a novel mutual consi...  \n",
              "3  consistency training has proven to be an advan...  \n",
              "4  to ensure safety in automated driving the corr...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ced7ec45-9333-4305-b76c-df806d07052e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "      <th>processed_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "      <td>stereo matching is one of the widely used tech...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "      <td>the recent advancements in artificial intellig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "      <td>in this paper we proposed a novel mutual consi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "      <td>consistency training has proven to be an advan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "      <td>to ensure safety in automated driving the corr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ced7ec45-9333-4305-b76c-df806d07052e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ced7ec45-9333-4305-b76c-df806d07052e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ced7ec45-9333-4305-b76c-df806d07052e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module\",\n          \"Studying the Plasticity in Deep Convolutional Neural Networks using Random Pruning\",\n          \"A Gentle Introduction to Deep Learning in Medical Image Processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"LIDAR point clouds and RGB-images are both extremely essential for 3D object\\ndetection. So many state-of-the-art 3D detection algorithms dedicate in fusing\\nthese two types of data effectively. However, their fusion methods based on\\nBirds Eye View (BEV) or voxel format are not accurate. In this paper, we\\npropose a novel fusion approach named Point-based Attentive Cont-conv\\nFusion(PACF) module, which fuses multi-sensor features directly on 3D points.\\nExcept for continuous convolution, we additionally add a Point-Pooling and an\\nAttentive Aggregation to make the fused features more expressive. Moreover,\\nbased on the PACF module, we propose a 3D multi-sensor multi-task network\\ncalled Pointcloud-Image RCNN(PI-RCNN as brief), which handles the image\\nsegmentation and 3D object detection tasks. PI-RCNN employs a segmentation\\nsub-network to extract full-resolution semantic feature maps from images and\\nthen fuses the multi-sensor features via powerful PACF module. Beneficial from\\nthe effectiveness of the PACF module and the expressive semantic features from\\nthe segmentation module, PI-RCNN can improve much in 3D object detection. We\\ndemonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3D\\nDetection benchmark, and our method can achieve state-of-the-art on the metric\\nof 3D AP.\",\n          \"Recently there has been a lot of work on pruning filters from deep\\nconvolutional neural networks (CNNs) with the intention of reducing\\ncomputations.The key idea is to rank the filters based on a certain criterion\\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\\nfilters are pruned away the remainder of the network is fine tuned and is shown\\nto give performance comparable to the original unpruned network. In this work,\\nwe report experiments which suggest that the comparable performance of the\\npruned network is not due to the specific criterion chosen but due to the\\ninherent plasticity of deep neural networks which allows them to recover from\\nthe loss of pruned filters once the rest of the filters are fine-tuned.\\nSpecifically we show counter-intuitive results wherein by randomly pruning\\n25-50% filters from deep CNNs we are able to obtain the same performance as\\nobtained by using state-of-the-art pruning methods. We empirically validate our\\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\\nneeds to be tested on only a small set of classes at test time (say, only\\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\\nclass specific pruning and show that even here a random pruning strategy gives\\nclose to state-of-the-art performance. Unlike existing approaches which mainly\\nfocus on the task of image classification, in this work we also report results\\non object detection and image segmentation. We show that using a simple random\\npruning strategy we can achieve significant speed up in object detection (74%\\nimprovement in fps) while retaining the same accuracy as that of the original\\nFaster RCNN model. Similarly we show that the performance of a pruned\\nSegmentation Network (SegNet) is actually very similar to that of the original\\nunpruned SegNet.\",\n          \"This paper tries to give a gentle introduction to deep learning in medical\\nimage processing, proceeding from theoretical foundations to applications. We\\nfirst discuss general reasons for the popularity of deep learning, including\\nseveral major breakthroughs in computer science. Next, we start reviewing the\\nfundamental basics of the perceptron and neural networks, along with some\\nfundamental theory that is often omitted. Doing so allows us to understand the\\nreasons for the rise of deep learning in many application domains. Obviously\\nmedical image processing is one of these areas which has been largely affected\\nby this rapid progress, in particular in image detection and recognition, image\\nsegmentation, image registration, and computer-aided diagnosis. There are also\\nrecent trends in physical simulation, modelling, and reconstruction that have\\nled to astonishing results. Yet, some of these approaches neglect prior\\nknowledge and hence bear the risk of producing implausible results. These\\napparent weaknesses highlight current limitations of deep learning. However, we\\nalso briefly discuss promising approaches that might be able to resolve these\\nproblems in the future.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 133,\n        \"samples\": [\n          \"['cs.CV', 'eess.IV', 'stat.AP', '62P99']\",\n          \"['cs.CV', 'cs.LG', 'I.2.1, I.4.6,']\",\n          \"['cs.CV', 'cs.CR', 'cs.LG']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"lidar point clouds and rgbimages are both extremely essential for 3d object detection so many stateoftheart 3d detection algorithms dedicate in fusing these two types of data effectively however their fusion methods based on birds eye view bev or voxel format are not accurate in this paper we propose a novel fusion approach named pointbased attentive contconv fusionpacf module which fuses multisensor features directly on 3d points except for continuous convolution we additionally add a pointpooling and an attentive aggregation to make the fused features more expressive moreover based on the pacf module we propose a 3d multisensor multitask network called pointcloudimage rcnnpircnn as brief which handles the image segmentation and 3d object detection tasks pircnn employs a segmentation subnetwork to extract fullresolution semantic feature maps from images and then fuses the multisensor features via powerful pacf module beneficial from the effectiveness of the pacf module and the expressive semantic features from the segmentation module pircnn can improve much in 3d object detection we demonstrate the effectiveness of the pacf module and pircnn on the kitti 3d detection benchmark and our method can achieve stateoftheart on the metric of 3d ap\",\n          \"recently there has been a lot of work on pruning filters from deep convolutional neural networks cnns with the intention of reducing computationsthe key idea is to rank the filters based on a certain criterion say l1norm and retain only the top ranked filters once the low scoring filters are pruned away the remainder of the network is fine tuned and is shown to give performance comparable to the original unpruned network in this work we report experiments which suggest that the comparable performance of the pruned network is not due to the specific criterion chosen but due to the inherent plasticity of deep neural networks which allows them to recover from the loss of pruned filters once the rest of the filters are finetuned specifically we show counterintuitive results wherein by randomly pruning 2550 filters from deep cnns we are able to obtain the same performance as obtained by using stateoftheart pruning methods we empirically validate our claims by doing an exhaustive evaluation with vgg16 and resnet50 we also evaluate a real world scenario where a cnn trained on all 1000 imagenet classes needs to be tested on only a small set of classes at test time say only animals we create a new benchmark dataset from imagenet to evaluate such class specific pruning and show that even here a random pruning strategy gives close to stateoftheart performance unlike existing approaches which mainly focus on the task of image classification in this work we also report results on object detection and image segmentation we show that using a simple random pruning strategy we can achieve significant speed up in object detection 74 improvement in fps while retaining the same accuracy as that of the original faster rcnn model similarly we show that the performance of a pruned segmentation network segnet is actually very similar to that of the original unpruned segnet\",\n          \"this paper tries to give a gentle introduction to deep learning in medical image processing proceeding from theoretical foundations to applications we first discuss general reasons for the popularity of deep learning including several major breakthroughs in computer science next we start reviewing the fundamental basics of the perceptron and neural networks along with some fundamental theory that is often omitted doing so allows us to understand the reasons for the rise of deep learning in many application domains obviously medical image processing is one of these areas which has been largely affected by this rapid progress in particular in image detection and recognition image segmentation image registration and computeraided diagnosis there are also recent trends in physical simulation modelling and reconstruction that have led to astonishing results yet some of these approaches neglect prior knowledge and hence bear the risk of producing implausible results these apparent weaknesses highlight current limitations of deep learning however we also briefly discuss promising approaches that might be able to resolve these problems in the future\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "data['tokenized_summaries'] = data['processed_summaries'].apply(word_tokenize)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "rJno7-dg2n_I",
        "outputId": "5d68ffc9-192b-462b-9147-01760f80d4bd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              titles  \\\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4  Background-Foreground Segmentation for Interio...   \n",
              "\n",
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                         terms  \\\n",
              "0           ['cs.CV', 'cs.LG']   \n",
              "1  ['cs.CV', 'cs.AI', 'cs.LG']   \n",
              "2           ['cs.CV', 'cs.AI']   \n",
              "3                    ['cs.CV']   \n",
              "4           ['cs.CV', 'cs.LG']   \n",
              "\n",
              "                                 processed_summaries  \\\n",
              "0  stereo matching is one of the widely used tech...   \n",
              "1  the recent advancements in artificial intellig...   \n",
              "2  in this paper we proposed a novel mutual consi...   \n",
              "3  consistency training has proven to be an advan...   \n",
              "4  to ensure safety in automated driving the corr...   \n",
              "\n",
              "                                 tokenized_summaries  \n",
              "0  [stereo, matching, is, one, of, the, widely, u...  \n",
              "1  [the, recent, advancements, in, artificial, in...  \n",
              "2  [in, this, paper, we, proposed, a, novel, mutu...  \n",
              "3  [consistency, training, has, proven, to, be, a...  \n",
              "4  [to, ensure, safety, in, automated, driving, t...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fab0023f-fec2-4cf8-88db-373db4c2b520\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "      <th>processed_summaries</th>\n",
              "      <th>tokenized_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "      <td>stereo matching is one of the widely used tech...</td>\n",
              "      <td>[stereo, matching, is, one, of, the, widely, u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "      <td>the recent advancements in artificial intellig...</td>\n",
              "      <td>[the, recent, advancements, in, artificial, in...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "      <td>in this paper we proposed a novel mutual consi...</td>\n",
              "      <td>[in, this, paper, we, proposed, a, novel, mutu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "      <td>consistency training has proven to be an advan...</td>\n",
              "      <td>[consistency, training, has, proven, to, be, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "      <td>to ensure safety in automated driving the corr...</td>\n",
              "      <td>[to, ensure, safety, in, automated, driving, t...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fab0023f-fec2-4cf8-88db-373db4c2b520')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fab0023f-fec2-4cf8-88db-373db4c2b520 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fab0023f-fec2-4cf8-88db-373db4c2b520');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module\",\n          \"Studying the Plasticity in Deep Convolutional Neural Networks using Random Pruning\",\n          \"A Gentle Introduction to Deep Learning in Medical Image Processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"LIDAR point clouds and RGB-images are both extremely essential for 3D object\\ndetection. So many state-of-the-art 3D detection algorithms dedicate in fusing\\nthese two types of data effectively. However, their fusion methods based on\\nBirds Eye View (BEV) or voxel format are not accurate. In this paper, we\\npropose a novel fusion approach named Point-based Attentive Cont-conv\\nFusion(PACF) module, which fuses multi-sensor features directly on 3D points.\\nExcept for continuous convolution, we additionally add a Point-Pooling and an\\nAttentive Aggregation to make the fused features more expressive. Moreover,\\nbased on the PACF module, we propose a 3D multi-sensor multi-task network\\ncalled Pointcloud-Image RCNN(PI-RCNN as brief), which handles the image\\nsegmentation and 3D object detection tasks. PI-RCNN employs a segmentation\\nsub-network to extract full-resolution semantic feature maps from images and\\nthen fuses the multi-sensor features via powerful PACF module. Beneficial from\\nthe effectiveness of the PACF module and the expressive semantic features from\\nthe segmentation module, PI-RCNN can improve much in 3D object detection. We\\ndemonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3D\\nDetection benchmark, and our method can achieve state-of-the-art on the metric\\nof 3D AP.\",\n          \"Recently there has been a lot of work on pruning filters from deep\\nconvolutional neural networks (CNNs) with the intention of reducing\\ncomputations.The key idea is to rank the filters based on a certain criterion\\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\\nfilters are pruned away the remainder of the network is fine tuned and is shown\\nto give performance comparable to the original unpruned network. In this work,\\nwe report experiments which suggest that the comparable performance of the\\npruned network is not due to the specific criterion chosen but due to the\\ninherent plasticity of deep neural networks which allows them to recover from\\nthe loss of pruned filters once the rest of the filters are fine-tuned.\\nSpecifically we show counter-intuitive results wherein by randomly pruning\\n25-50% filters from deep CNNs we are able to obtain the same performance as\\nobtained by using state-of-the-art pruning methods. We empirically validate our\\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\\nneeds to be tested on only a small set of classes at test time (say, only\\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\\nclass specific pruning and show that even here a random pruning strategy gives\\nclose to state-of-the-art performance. Unlike existing approaches which mainly\\nfocus on the task of image classification, in this work we also report results\\non object detection and image segmentation. We show that using a simple random\\npruning strategy we can achieve significant speed up in object detection (74%\\nimprovement in fps) while retaining the same accuracy as that of the original\\nFaster RCNN model. Similarly we show that the performance of a pruned\\nSegmentation Network (SegNet) is actually very similar to that of the original\\nunpruned SegNet.\",\n          \"This paper tries to give a gentle introduction to deep learning in medical\\nimage processing, proceeding from theoretical foundations to applications. We\\nfirst discuss general reasons for the popularity of deep learning, including\\nseveral major breakthroughs in computer science. Next, we start reviewing the\\nfundamental basics of the perceptron and neural networks, along with some\\nfundamental theory that is often omitted. Doing so allows us to understand the\\nreasons for the rise of deep learning in many application domains. Obviously\\nmedical image processing is one of these areas which has been largely affected\\nby this rapid progress, in particular in image detection and recognition, image\\nsegmentation, image registration, and computer-aided diagnosis. There are also\\nrecent trends in physical simulation, modelling, and reconstruction that have\\nled to astonishing results. Yet, some of these approaches neglect prior\\nknowledge and hence bear the risk of producing implausible results. These\\napparent weaknesses highlight current limitations of deep learning. However, we\\nalso briefly discuss promising approaches that might be able to resolve these\\nproblems in the future.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 133,\n        \"samples\": [\n          \"['cs.CV', 'eess.IV', 'stat.AP', '62P99']\",\n          \"['cs.CV', 'cs.LG', 'I.2.1, I.4.6,']\",\n          \"['cs.CV', 'cs.CR', 'cs.LG']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"lidar point clouds and rgbimages are both extremely essential for 3d object detection so many stateoftheart 3d detection algorithms dedicate in fusing these two types of data effectively however their fusion methods based on birds eye view bev or voxel format are not accurate in this paper we propose a novel fusion approach named pointbased attentive contconv fusionpacf module which fuses multisensor features directly on 3d points except for continuous convolution we additionally add a pointpooling and an attentive aggregation to make the fused features more expressive moreover based on the pacf module we propose a 3d multisensor multitask network called pointcloudimage rcnnpircnn as brief which handles the image segmentation and 3d object detection tasks pircnn employs a segmentation subnetwork to extract fullresolution semantic feature maps from images and then fuses the multisensor features via powerful pacf module beneficial from the effectiveness of the pacf module and the expressive semantic features from the segmentation module pircnn can improve much in 3d object detection we demonstrate the effectiveness of the pacf module and pircnn on the kitti 3d detection benchmark and our method can achieve stateoftheart on the metric of 3d ap\",\n          \"recently there has been a lot of work on pruning filters from deep convolutional neural networks cnns with the intention of reducing computationsthe key idea is to rank the filters based on a certain criterion say l1norm and retain only the top ranked filters once the low scoring filters are pruned away the remainder of the network is fine tuned and is shown to give performance comparable to the original unpruned network in this work we report experiments which suggest that the comparable performance of the pruned network is not due to the specific criterion chosen but due to the inherent plasticity of deep neural networks which allows them to recover from the loss of pruned filters once the rest of the filters are finetuned specifically we show counterintuitive results wherein by randomly pruning 2550 filters from deep cnns we are able to obtain the same performance as obtained by using stateoftheart pruning methods we empirically validate our claims by doing an exhaustive evaluation with vgg16 and resnet50 we also evaluate a real world scenario where a cnn trained on all 1000 imagenet classes needs to be tested on only a small set of classes at test time say only animals we create a new benchmark dataset from imagenet to evaluate such class specific pruning and show that even here a random pruning strategy gives close to stateoftheart performance unlike existing approaches which mainly focus on the task of image classification in this work we also report results on object detection and image segmentation we show that using a simple random pruning strategy we can achieve significant speed up in object detection 74 improvement in fps while retaining the same accuracy as that of the original faster rcnn model similarly we show that the performance of a pruned segmentation network segnet is actually very similar to that of the original unpruned segnet\",\n          \"this paper tries to give a gentle introduction to deep learning in medical image processing proceeding from theoretical foundations to applications we first discuss general reasons for the popularity of deep learning including several major breakthroughs in computer science next we start reviewing the fundamental basics of the perceptron and neural networks along with some fundamental theory that is often omitted doing so allows us to understand the reasons for the rise of deep learning in many application domains obviously medical image processing is one of these areas which has been largely affected by this rapid progress in particular in image detection and recognition image segmentation image registration and computeraided diagnosis there are also recent trends in physical simulation modelling and reconstruction that have led to astonishing results yet some of these approaches neglect prior knowledge and hence bear the risk of producing implausible results these apparent weaknesses highlight current limitations of deep learning however we also briefly discuss promising approaches that might be able to resolve these problems in the future\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(token_list):\n",
        "    return [word for word in token_list if word not in stop_words]\n",
        "data['filtered_summaries'] = data['tokenized_summaries'].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WP6xGkV322h8",
        "outputId": "5f037953-b0c5-43f2-8ec5-05d9d0b71c6e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_tokens(token_list):\n",
        "    return [lemmatizer.lemmatize(word) for word in token_list]\n",
        "data['lemmatized_summaries'] = data['filtered_summaries'].apply(lemmatize_tokens)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "CfZJtox022LV",
        "outputId": "7f611f42-6285-4df4-8bb2-2e823ec92280"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              titles  \\\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4  Background-Foreground Segmentation for Interio...   \n",
              "\n",
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                         terms  \\\n",
              "0           ['cs.CV', 'cs.LG']   \n",
              "1  ['cs.CV', 'cs.AI', 'cs.LG']   \n",
              "2           ['cs.CV', 'cs.AI']   \n",
              "3                    ['cs.CV']   \n",
              "4           ['cs.CV', 'cs.LG']   \n",
              "\n",
              "                                 processed_summaries  \\\n",
              "0  stereo matching is one of the widely used tech...   \n",
              "1  the recent advancements in artificial intellig...   \n",
              "2  in this paper we proposed a novel mutual consi...   \n",
              "3  consistency training has proven to be an advan...   \n",
              "4  to ensure safety in automated driving the corr...   \n",
              "\n",
              "                                 tokenized_summaries  \\\n",
              "0  [stereo, matching, is, one, of, the, widely, u...   \n",
              "1  [the, recent, advancements, in, artificial, in...   \n",
              "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
              "3  [consistency, training, has, proven, to, be, a...   \n",
              "4  [to, ensure, safety, in, automated, driving, t...   \n",
              "\n",
              "                                  filtered_summaries  \\\n",
              "0  [stereo, matching, one, widely, used, techniqu...   \n",
              "1  [recent, advancements, artificial, intelligenc...   \n",
              "2  [paper, proposed, novel, mutual, consistency, ...   \n",
              "3  [consistency, training, proven, advanced, semi...   \n",
              "4  [ensure, safety, automated, driving, correct, ...   \n",
              "\n",
              "                                lemmatized_summaries  \n",
              "0  [stereo, matching, one, widely, used, techniqu...  \n",
              "1  [recent, advancement, artificial, intelligence...  \n",
              "2  [paper, proposed, novel, mutual, consistency, ...  \n",
              "3  [consistency, training, proven, advanced, semi...  \n",
              "4  [ensure, safety, automated, driving, correct, ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e9ebe403-46f6-4c8e-bd01-f5ff4040ceff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "      <th>processed_summaries</th>\n",
              "      <th>tokenized_summaries</th>\n",
              "      <th>filtered_summaries</th>\n",
              "      <th>lemmatized_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "      <td>stereo matching is one of the widely used tech...</td>\n",
              "      <td>[stereo, matching, is, one, of, the, widely, u...</td>\n",
              "      <td>[stereo, matching, one, widely, used, techniqu...</td>\n",
              "      <td>[stereo, matching, one, widely, used, techniqu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "      <td>the recent advancements in artificial intellig...</td>\n",
              "      <td>[the, recent, advancements, in, artificial, in...</td>\n",
              "      <td>[recent, advancements, artificial, intelligenc...</td>\n",
              "      <td>[recent, advancement, artificial, intelligence...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "      <td>in this paper we proposed a novel mutual consi...</td>\n",
              "      <td>[in, this, paper, we, proposed, a, novel, mutu...</td>\n",
              "      <td>[paper, proposed, novel, mutual, consistency, ...</td>\n",
              "      <td>[paper, proposed, novel, mutual, consistency, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "      <td>consistency training has proven to be an advan...</td>\n",
              "      <td>[consistency, training, has, proven, to, be, a...</td>\n",
              "      <td>[consistency, training, proven, advanced, semi...</td>\n",
              "      <td>[consistency, training, proven, advanced, semi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "      <td>to ensure safety in automated driving the corr...</td>\n",
              "      <td>[to, ensure, safety, in, automated, driving, t...</td>\n",
              "      <td>[ensure, safety, automated, driving, correct, ...</td>\n",
              "      <td>[ensure, safety, automated, driving, correct, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e9ebe403-46f6-4c8e-bd01-f5ff4040ceff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e9ebe403-46f6-4c8e-bd01-f5ff4040ceff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e9ebe403-46f6-4c8e-bd01-f5ff4040ceff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module\",\n          \"Studying the Plasticity in Deep Convolutional Neural Networks using Random Pruning\",\n          \"A Gentle Introduction to Deep Learning in Medical Image Processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"LIDAR point clouds and RGB-images are both extremely essential for 3D object\\ndetection. So many state-of-the-art 3D detection algorithms dedicate in fusing\\nthese two types of data effectively. However, their fusion methods based on\\nBirds Eye View (BEV) or voxel format are not accurate. In this paper, we\\npropose a novel fusion approach named Point-based Attentive Cont-conv\\nFusion(PACF) module, which fuses multi-sensor features directly on 3D points.\\nExcept for continuous convolution, we additionally add a Point-Pooling and an\\nAttentive Aggregation to make the fused features more expressive. Moreover,\\nbased on the PACF module, we propose a 3D multi-sensor multi-task network\\ncalled Pointcloud-Image RCNN(PI-RCNN as brief), which handles the image\\nsegmentation and 3D object detection tasks. PI-RCNN employs a segmentation\\nsub-network to extract full-resolution semantic feature maps from images and\\nthen fuses the multi-sensor features via powerful PACF module. Beneficial from\\nthe effectiveness of the PACF module and the expressive semantic features from\\nthe segmentation module, PI-RCNN can improve much in 3D object detection. We\\ndemonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3D\\nDetection benchmark, and our method can achieve state-of-the-art on the metric\\nof 3D AP.\",\n          \"Recently there has been a lot of work on pruning filters from deep\\nconvolutional neural networks (CNNs) with the intention of reducing\\ncomputations.The key idea is to rank the filters based on a certain criterion\\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\\nfilters are pruned away the remainder of the network is fine tuned and is shown\\nto give performance comparable to the original unpruned network. In this work,\\nwe report experiments which suggest that the comparable performance of the\\npruned network is not due to the specific criterion chosen but due to the\\ninherent plasticity of deep neural networks which allows them to recover from\\nthe loss of pruned filters once the rest of the filters are fine-tuned.\\nSpecifically we show counter-intuitive results wherein by randomly pruning\\n25-50% filters from deep CNNs we are able to obtain the same performance as\\nobtained by using state-of-the-art pruning methods. We empirically validate our\\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\\nneeds to be tested on only a small set of classes at test time (say, only\\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\\nclass specific pruning and show that even here a random pruning strategy gives\\nclose to state-of-the-art performance. Unlike existing approaches which mainly\\nfocus on the task of image classification, in this work we also report results\\non object detection and image segmentation. We show that using a simple random\\npruning strategy we can achieve significant speed up in object detection (74%\\nimprovement in fps) while retaining the same accuracy as that of the original\\nFaster RCNN model. Similarly we show that the performance of a pruned\\nSegmentation Network (SegNet) is actually very similar to that of the original\\nunpruned SegNet.\",\n          \"This paper tries to give a gentle introduction to deep learning in medical\\nimage processing, proceeding from theoretical foundations to applications. We\\nfirst discuss general reasons for the popularity of deep learning, including\\nseveral major breakthroughs in computer science. Next, we start reviewing the\\nfundamental basics of the perceptron and neural networks, along with some\\nfundamental theory that is often omitted. Doing so allows us to understand the\\nreasons for the rise of deep learning in many application domains. Obviously\\nmedical image processing is one of these areas which has been largely affected\\nby this rapid progress, in particular in image detection and recognition, image\\nsegmentation, image registration, and computer-aided diagnosis. There are also\\nrecent trends in physical simulation, modelling, and reconstruction that have\\nled to astonishing results. Yet, some of these approaches neglect prior\\nknowledge and hence bear the risk of producing implausible results. These\\napparent weaknesses highlight current limitations of deep learning. However, we\\nalso briefly discuss promising approaches that might be able to resolve these\\nproblems in the future.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 133,\n        \"samples\": [\n          \"['cs.CV', 'eess.IV', 'stat.AP', '62P99']\",\n          \"['cs.CV', 'cs.LG', 'I.2.1, I.4.6,']\",\n          \"['cs.CV', 'cs.CR', 'cs.LG']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"lidar point clouds and rgbimages are both extremely essential for 3d object detection so many stateoftheart 3d detection algorithms dedicate in fusing these two types of data effectively however their fusion methods based on birds eye view bev or voxel format are not accurate in this paper we propose a novel fusion approach named pointbased attentive contconv fusionpacf module which fuses multisensor features directly on 3d points except for continuous convolution we additionally add a pointpooling and an attentive aggregation to make the fused features more expressive moreover based on the pacf module we propose a 3d multisensor multitask network called pointcloudimage rcnnpircnn as brief which handles the image segmentation and 3d object detection tasks pircnn employs a segmentation subnetwork to extract fullresolution semantic feature maps from images and then fuses the multisensor features via powerful pacf module beneficial from the effectiveness of the pacf module and the expressive semantic features from the segmentation module pircnn can improve much in 3d object detection we demonstrate the effectiveness of the pacf module and pircnn on the kitti 3d detection benchmark and our method can achieve stateoftheart on the metric of 3d ap\",\n          \"recently there has been a lot of work on pruning filters from deep convolutional neural networks cnns with the intention of reducing computationsthe key idea is to rank the filters based on a certain criterion say l1norm and retain only the top ranked filters once the low scoring filters are pruned away the remainder of the network is fine tuned and is shown to give performance comparable to the original unpruned network in this work we report experiments which suggest that the comparable performance of the pruned network is not due to the specific criterion chosen but due to the inherent plasticity of deep neural networks which allows them to recover from the loss of pruned filters once the rest of the filters are finetuned specifically we show counterintuitive results wherein by randomly pruning 2550 filters from deep cnns we are able to obtain the same performance as obtained by using stateoftheart pruning methods we empirically validate our claims by doing an exhaustive evaluation with vgg16 and resnet50 we also evaluate a real world scenario where a cnn trained on all 1000 imagenet classes needs to be tested on only a small set of classes at test time say only animals we create a new benchmark dataset from imagenet to evaluate such class specific pruning and show that even here a random pruning strategy gives close to stateoftheart performance unlike existing approaches which mainly focus on the task of image classification in this work we also report results on object detection and image segmentation we show that using a simple random pruning strategy we can achieve significant speed up in object detection 74 improvement in fps while retaining the same accuracy as that of the original faster rcnn model similarly we show that the performance of a pruned segmentation network segnet is actually very similar to that of the original unpruned segnet\",\n          \"this paper tries to give a gentle introduction to deep learning in medical image processing proceeding from theoretical foundations to applications we first discuss general reasons for the popularity of deep learning including several major breakthroughs in computer science next we start reviewing the fundamental basics of the perceptron and neural networks along with some fundamental theory that is often omitted doing so allows us to understand the reasons for the rise of deep learning in many application domains obviously medical image processing is one of these areas which has been largely affected by this rapid progress in particular in image detection and recognition image segmentation image registration and computeraided diagnosis there are also recent trends in physical simulation modelling and reconstruction that have led to astonishing results yet some of these approaches neglect prior knowledge and hence bear the risk of producing implausible results these apparent weaknesses highlight current limitations of deep learning however we also briefly discuss promising approaches that might be able to resolve these problems in the future\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filtered_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rejoin_tokens(token_list):\n",
        "    return ' '.join(token_list)\n",
        "data['clean_summaries'] = data['lemmatized_summaries'].apply(rejoin_tokens)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "-Fb0Amk-2-XT",
        "outputId": "f897133d-fac2-441c-f7ed-d6a9a1313650"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              titles  \\\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4  Background-Foreground Segmentation for Interio...   \n",
              "\n",
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                         terms  \\\n",
              "0           ['cs.CV', 'cs.LG']   \n",
              "1  ['cs.CV', 'cs.AI', 'cs.LG']   \n",
              "2           ['cs.CV', 'cs.AI']   \n",
              "3                    ['cs.CV']   \n",
              "4           ['cs.CV', 'cs.LG']   \n",
              "\n",
              "                                 processed_summaries  \\\n",
              "0  stereo matching is one of the widely used tech...   \n",
              "1  the recent advancements in artificial intellig...   \n",
              "2  in this paper we proposed a novel mutual consi...   \n",
              "3  consistency training has proven to be an advan...   \n",
              "4  to ensure safety in automated driving the corr...   \n",
              "\n",
              "                                 tokenized_summaries  \\\n",
              "0  [stereo, matching, is, one, of, the, widely, u...   \n",
              "1  [the, recent, advancements, in, artificial, in...   \n",
              "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
              "3  [consistency, training, has, proven, to, be, a...   \n",
              "4  [to, ensure, safety, in, automated, driving, t...   \n",
              "\n",
              "                                  filtered_summaries  \\\n",
              "0  [stereo, matching, one, widely, used, techniqu...   \n",
              "1  [recent, advancements, artificial, intelligenc...   \n",
              "2  [paper, proposed, novel, mutual, consistency, ...   \n",
              "3  [consistency, training, proven, advanced, semi...   \n",
              "4  [ensure, safety, automated, driving, correct, ...   \n",
              "\n",
              "                                lemmatized_summaries  \\\n",
              "0  [stereo, matching, one, widely, used, techniqu...   \n",
              "1  [recent, advancement, artificial, intelligence...   \n",
              "2  [paper, proposed, novel, mutual, consistency, ...   \n",
              "3  [consistency, training, proven, advanced, semi...   \n",
              "4  [ensure, safety, automated, driving, correct, ...   \n",
              "\n",
              "                                     clean_summaries  \n",
              "0  stereo matching one widely used technique infe...  \n",
              "1  recent advancement artificial intelligence ai ...  \n",
              "2  paper proposed novel mutual consistency networ...  \n",
              "3  consistency training proven advanced semisuper...  \n",
              "4  ensure safety automated driving correct percep...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e15e54b1-4dfd-41d6-bfc2-f6afa42707c8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "      <th>processed_summaries</th>\n",
              "      <th>tokenized_summaries</th>\n",
              "      <th>filtered_summaries</th>\n",
              "      <th>lemmatized_summaries</th>\n",
              "      <th>clean_summaries</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "      <td>stereo matching is one of the widely used tech...</td>\n",
              "      <td>[stereo, matching, is, one, of, the, widely, u...</td>\n",
              "      <td>[stereo, matching, one, widely, used, techniqu...</td>\n",
              "      <td>[stereo, matching, one, widely, used, techniqu...</td>\n",
              "      <td>stereo matching one widely used technique infe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "      <td>the recent advancements in artificial intellig...</td>\n",
              "      <td>[the, recent, advancements, in, artificial, in...</td>\n",
              "      <td>[recent, advancements, artificial, intelligenc...</td>\n",
              "      <td>[recent, advancement, artificial, intelligence...</td>\n",
              "      <td>recent advancement artificial intelligence ai ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "      <td>in this paper we proposed a novel mutual consi...</td>\n",
              "      <td>[in, this, paper, we, proposed, a, novel, mutu...</td>\n",
              "      <td>[paper, proposed, novel, mutual, consistency, ...</td>\n",
              "      <td>[paper, proposed, novel, mutual, consistency, ...</td>\n",
              "      <td>paper proposed novel mutual consistency networ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "      <td>consistency training has proven to be an advan...</td>\n",
              "      <td>[consistency, training, has, proven, to, be, a...</td>\n",
              "      <td>[consistency, training, proven, advanced, semi...</td>\n",
              "      <td>[consistency, training, proven, advanced, semi...</td>\n",
              "      <td>consistency training proven advanced semisuper...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "      <td>to ensure safety in automated driving the corr...</td>\n",
              "      <td>[to, ensure, safety, in, automated, driving, t...</td>\n",
              "      <td>[ensure, safety, automated, driving, correct, ...</td>\n",
              "      <td>[ensure, safety, automated, driving, correct, ...</td>\n",
              "      <td>ensure safety automated driving correct percep...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e15e54b1-4dfd-41d6-bfc2-f6afa42707c8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e15e54b1-4dfd-41d6-bfc2-f6afa42707c8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e15e54b1-4dfd-41d6-bfc2-f6afa42707c8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module\",\n          \"Studying the Plasticity in Deep Convolutional Neural Networks using Random Pruning\",\n          \"A Gentle Introduction to Deep Learning in Medical Image Processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"LIDAR point clouds and RGB-images are both extremely essential for 3D object\\ndetection. So many state-of-the-art 3D detection algorithms dedicate in fusing\\nthese two types of data effectively. However, their fusion methods based on\\nBirds Eye View (BEV) or voxel format are not accurate. In this paper, we\\npropose a novel fusion approach named Point-based Attentive Cont-conv\\nFusion(PACF) module, which fuses multi-sensor features directly on 3D points.\\nExcept for continuous convolution, we additionally add a Point-Pooling and an\\nAttentive Aggregation to make the fused features more expressive. Moreover,\\nbased on the PACF module, we propose a 3D multi-sensor multi-task network\\ncalled Pointcloud-Image RCNN(PI-RCNN as brief), which handles the image\\nsegmentation and 3D object detection tasks. PI-RCNN employs a segmentation\\nsub-network to extract full-resolution semantic feature maps from images and\\nthen fuses the multi-sensor features via powerful PACF module. Beneficial from\\nthe effectiveness of the PACF module and the expressive semantic features from\\nthe segmentation module, PI-RCNN can improve much in 3D object detection. We\\ndemonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3D\\nDetection benchmark, and our method can achieve state-of-the-art on the metric\\nof 3D AP.\",\n          \"Recently there has been a lot of work on pruning filters from deep\\nconvolutional neural networks (CNNs) with the intention of reducing\\ncomputations.The key idea is to rank the filters based on a certain criterion\\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\\nfilters are pruned away the remainder of the network is fine tuned and is shown\\nto give performance comparable to the original unpruned network. In this work,\\nwe report experiments which suggest that the comparable performance of the\\npruned network is not due to the specific criterion chosen but due to the\\ninherent plasticity of deep neural networks which allows them to recover from\\nthe loss of pruned filters once the rest of the filters are fine-tuned.\\nSpecifically we show counter-intuitive results wherein by randomly pruning\\n25-50% filters from deep CNNs we are able to obtain the same performance as\\nobtained by using state-of-the-art pruning methods. We empirically validate our\\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\\nneeds to be tested on only a small set of classes at test time (say, only\\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\\nclass specific pruning and show that even here a random pruning strategy gives\\nclose to state-of-the-art performance. Unlike existing approaches which mainly\\nfocus on the task of image classification, in this work we also report results\\non object detection and image segmentation. We show that using a simple random\\npruning strategy we can achieve significant speed up in object detection (74%\\nimprovement in fps) while retaining the same accuracy as that of the original\\nFaster RCNN model. Similarly we show that the performance of a pruned\\nSegmentation Network (SegNet) is actually very similar to that of the original\\nunpruned SegNet.\",\n          \"This paper tries to give a gentle introduction to deep learning in medical\\nimage processing, proceeding from theoretical foundations to applications. We\\nfirst discuss general reasons for the popularity of deep learning, including\\nseveral major breakthroughs in computer science. Next, we start reviewing the\\nfundamental basics of the perceptron and neural networks, along with some\\nfundamental theory that is often omitted. Doing so allows us to understand the\\nreasons for the rise of deep learning in many application domains. Obviously\\nmedical image processing is one of these areas which has been largely affected\\nby this rapid progress, in particular in image detection and recognition, image\\nsegmentation, image registration, and computer-aided diagnosis. There are also\\nrecent trends in physical simulation, modelling, and reconstruction that have\\nled to astonishing results. Yet, some of these approaches neglect prior\\nknowledge and hence bear the risk of producing implausible results. These\\napparent weaknesses highlight current limitations of deep learning. However, we\\nalso briefly discuss promising approaches that might be able to resolve these\\nproblems in the future.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 133,\n        \"samples\": [\n          \"['cs.CV', 'eess.IV', 'stat.AP', '62P99']\",\n          \"['cs.CV', 'cs.LG', 'I.2.1, I.4.6,']\",\n          \"['cs.CV', 'cs.CR', 'cs.LG']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"lidar point clouds and rgbimages are both extremely essential for 3d object detection so many stateoftheart 3d detection algorithms dedicate in fusing these two types of data effectively however their fusion methods based on birds eye view bev or voxel format are not accurate in this paper we propose a novel fusion approach named pointbased attentive contconv fusionpacf module which fuses multisensor features directly on 3d points except for continuous convolution we additionally add a pointpooling and an attentive aggregation to make the fused features more expressive moreover based on the pacf module we propose a 3d multisensor multitask network called pointcloudimage rcnnpircnn as brief which handles the image segmentation and 3d object detection tasks pircnn employs a segmentation subnetwork to extract fullresolution semantic feature maps from images and then fuses the multisensor features via powerful pacf module beneficial from the effectiveness of the pacf module and the expressive semantic features from the segmentation module pircnn can improve much in 3d object detection we demonstrate the effectiveness of the pacf module and pircnn on the kitti 3d detection benchmark and our method can achieve stateoftheart on the metric of 3d ap\",\n          \"recently there has been a lot of work on pruning filters from deep convolutional neural networks cnns with the intention of reducing computationsthe key idea is to rank the filters based on a certain criterion say l1norm and retain only the top ranked filters once the low scoring filters are pruned away the remainder of the network is fine tuned and is shown to give performance comparable to the original unpruned network in this work we report experiments which suggest that the comparable performance of the pruned network is not due to the specific criterion chosen but due to the inherent plasticity of deep neural networks which allows them to recover from the loss of pruned filters once the rest of the filters are finetuned specifically we show counterintuitive results wherein by randomly pruning 2550 filters from deep cnns we are able to obtain the same performance as obtained by using stateoftheart pruning methods we empirically validate our claims by doing an exhaustive evaluation with vgg16 and resnet50 we also evaluate a real world scenario where a cnn trained on all 1000 imagenet classes needs to be tested on only a small set of classes at test time say only animals we create a new benchmark dataset from imagenet to evaluate such class specific pruning and show that even here a random pruning strategy gives close to stateoftheart performance unlike existing approaches which mainly focus on the task of image classification in this work we also report results on object detection and image segmentation we show that using a simple random pruning strategy we can achieve significant speed up in object detection 74 improvement in fps while retaining the same accuracy as that of the original faster rcnn model similarly we show that the performance of a pruned segmentation network segnet is actually very similar to that of the original unpruned segnet\",\n          \"this paper tries to give a gentle introduction to deep learning in medical image processing proceeding from theoretical foundations to applications we first discuss general reasons for the popularity of deep learning including several major breakthroughs in computer science next we start reviewing the fundamental basics of the perceptron and neural networks along with some fundamental theory that is often omitted doing so allows us to understand the reasons for the rise of deep learning in many application domains obviously medical image processing is one of these areas which has been largely affected by this rapid progress in particular in image detection and recognition image segmentation image registration and computeraided diagnosis there are also recent trends in physical simulation modelling and reconstruction that have led to astonishing results yet some of these approaches neglect prior knowledge and hence bear the risk of producing implausible results these apparent weaknesses highlight current limitations of deep learning however we also briefly discuss promising approaches that might be able to resolve these problems in the future\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filtered_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"lidar point cloud rgbimages extremely essential 3d object detection many stateoftheart 3d detection algorithm dedicate fusing two type data effectively however fusion method based bird eye view bev voxel format accurate paper propose novel fusion approach named pointbased attentive contconv fusionpacf module fuse multisensor feature directly 3d point except continuous convolution additionally add pointpooling attentive aggregation make fused feature expressive moreover based pacf module propose 3d multisensor multitask network called pointcloudimage rcnnpircnn brief handle image segmentation 3d object detection task pircnn employ segmentation subnetwork extract fullresolution semantic feature map image fuse multisensor feature via powerful pacf module beneficial effectiveness pacf module expressive semantic feature segmentation module pircnn improve much 3d object detection demonstrate effectiveness pacf module pircnn kitti 3d detection benchmark method achieve stateoftheart metric 3d ap\",\n          \"recently lot work pruning filter deep convolutional neural network cnns intention reducing computationsthe key idea rank filter based certain criterion say l1norm retain top ranked filter low scoring filter pruned away remainder network fine tuned shown give performance comparable original unpruned network work report experiment suggest comparable performance pruned network due specific criterion chosen due inherent plasticity deep neural network allows recover loss pruned filter rest filter finetuned specifically show counterintuitive result wherein randomly pruning 2550 filter deep cnns able obtain performance obtained using stateoftheart pruning method empirically validate claim exhaustive evaluation vgg16 resnet50 also evaluate real world scenario cnn trained 1000 imagenet class need tested small set class test time say animal create new benchmark dataset imagenet evaluate class specific pruning show even random pruning strategy give close stateoftheart performance unlike existing approach mainly focus task image classification work also report result object detection image segmentation show using simple random pruning strategy achieve significant speed object detection 74 improvement fps retaining accuracy original faster rcnn model similarly show performance pruned segmentation network segnet actually similar original unpruned segnet\",\n          \"paper try give gentle introduction deep learning medical image processing proceeding theoretical foundation application first discus general reason popularity deep learning including several major breakthrough computer science next start reviewing fundamental basic perceptron neural network along fundamental theory often omitted allows u understand reason rise deep learning many application domain obviously medical image processing one area largely affected rapid progress particular image detection recognition image segmentation image registration computeraided diagnosis also recent trend physical simulation modelling reconstruction led astonishing result yet approach neglect prior knowledge hence bear risk producing implausible result apparent weakness highlight current limitation deep learning however also briefly discus promising approach might able resolve problem future\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "def nltk_preprocessing_pipeline(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\"\n",
        "        \"\\U0001F300-\\U0001F5FF\"\n",
        "        \"\\U0001F680-\\U0001F6FF\"\n",
        "        \"\\U0001F1E0-\\U0001F1FF\"\n",
        "        \"\\U00002702-\\U000027B0\"\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"\\U0001f926-\\U0001f937\"\n",
        "        \"\\U00010000-\\U0010ffff\"\n",
        "        \"\\u2640-\\u2642\"\n",
        "        \"\\u2600-\\u2B55\"\n",
        "        \"\\u200d\"\n",
        "        \"\\u23cf\"\n",
        "        \"\\u23e9\"\n",
        "        \"\\u231a\"\n",
        "        \"\\ufe0f\"\n",
        "        \"\\u3030\"\n",
        "        \"]+\", re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "data['clean_summaries_pipeline'] = data['summaries'].apply(nltk_preprocessing_pipeline)\n",
        "\n",
        "comparison = data[['clean_summaries', 'clean_summaries_pipeline']].head(10)\n",
        "print(\"Comparison of step-by-step vs pipeline approach:\")\n",
        "print(comparison)\n",
        "print(\"\\nConsistency check (first 5 rows):\")\n",
        "for i in range(min(5, len(data))):\n",
        "    if data['clean_summaries'].iloc[i] == data['clean_summaries_pipeline'].iloc[i]:\n",
        "        print(f\"Row {i}: MATCH\")\n",
        "    else:\n",
        "        print(f\"Row {i}: MISMATCH\")\n",
        "        print(f\"  Step-by-step: {data['clean_summaries'].iloc[i][:100]}\")\n",
        "        print(f\"  Pipeline:     {data['clean_summaries_pipeline'].iloc[i][:100]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6pEMdhp3B8f",
        "outputId": "d17e71ea-5ad5-4471-b54a-ee10cc7ba50b"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparison of step-by-step vs pipeline approach:\n",
            "                                     clean_summaries  \\\n",
            "0  stereo matching one widely used technique infe...   \n",
            "1  recent advancement artificial intelligence ai ...   \n",
            "2  paper proposed novel mutual consistency networ...   \n",
            "3  consistency training proven advanced semisuper...   \n",
            "4  ensure safety automated driving correct percep...   \n",
            "5  highquality training data play key role image ...   \n",
            "6  semantic segmentation fineresolution urban sce...   \n",
            "7  mitigate radiologist workload computeraided di...   \n",
            "8  generalising deep model new data new centre te...   \n",
            "9  success deep learning method medical image seg...   \n",
            "\n",
            "                            clean_summaries_pipeline  \n",
            "0  stereo matching one widely used technique infe...  \n",
            "1  recent advancement artificial intelligence ai ...  \n",
            "2  paper proposed novel mutual consistency networ...  \n",
            "3  consistency training proven advanced semisuper...  \n",
            "4  ensure safety automated driving correct percep...  \n",
            "5  highquality training data play key role image ...  \n",
            "6  semantic segmentation fineresolution urban sce...  \n",
            "7  mitigate radiologist workload computeraided di...  \n",
            "8  generalising deep model new data new centre te...  \n",
            "9  success deep learning method medical image seg...  \n",
            "\n",
            "Consistency check (first 5 rows):\n",
            "Row 0: MATCH\n",
            "Row 1: MATCH\n",
            "Row 2: MATCH\n",
            "Row 3: MATCH\n",
            "Row 4: MATCH\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_preprocessing_pipeline(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    emoji_pattern = re.compile(\n",
        "        \"[\"\n",
        "        \"\\U0001F600-\\U0001F64F\"\n",
        "        \"\\U0001F300-\\U0001F5FF\"\n",
        "        \"\\U0001F680-\\U0001F6FF\"\n",
        "        \"\\U0001F1E0-\\U0001F1FF\"\n",
        "        \"\\U00002702-\\U000027B0\"\n",
        "        \"\\U000024C2-\\U0001F251\"\n",
        "        \"\\U0001f926-\\U0001f937\"\n",
        "        \"\\U00010000-\\U0010ffff\"\n",
        "        \"\\u2640-\\u2642\"\n",
        "        \"\\u2600-\\u2B55\"\n",
        "        \"\\u200d\"\n",
        "        \"\\u23cf\"\n",
        "        \"\\u23e9\"\n",
        "        \"\\u231a\"\n",
        "        \"\\ufe0f\"\n",
        "        \"\\u3030\"\n",
        "        \"]+\", re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    doc = nlp(text)\n",
        "    lemmatized_tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
        "\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "data['clean_summaries_spacy'] = data['summaries'].apply(spacy_preprocessing_pipeline)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591
        },
        "id": "xUrJpLrW3KA3",
        "outputId": "aff5f3d7-41f0-44d0-80c3-4d24b7f46528"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              titles  \\\n",
              "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
              "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
              "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
              "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
              "4  Background-Foreground Segmentation for Interio...   \n",
              "\n",
              "                                           summaries  \\\n",
              "0  Stereo matching is one of the widely used tech...   \n",
              "1  The recent advancements in artificial intellig...   \n",
              "2  In this paper, we proposed a novel mutual cons...   \n",
              "3  Consistency training has proven to be an advan...   \n",
              "4  To ensure safety in automated driving, the cor...   \n",
              "\n",
              "                         terms  \\\n",
              "0           ['cs.CV', 'cs.LG']   \n",
              "1  ['cs.CV', 'cs.AI', 'cs.LG']   \n",
              "2           ['cs.CV', 'cs.AI']   \n",
              "3                    ['cs.CV']   \n",
              "4           ['cs.CV', 'cs.LG']   \n",
              "\n",
              "                                 processed_summaries  \\\n",
              "0  stereo matching is one of the widely used tech...   \n",
              "1  the recent advancements in artificial intellig...   \n",
              "2  in this paper we proposed a novel mutual consi...   \n",
              "3  consistency training has proven to be an advan...   \n",
              "4  to ensure safety in automated driving the corr...   \n",
              "\n",
              "                                 tokenized_summaries  \\\n",
              "0  [stereo, matching, is, one, of, the, widely, u...   \n",
              "1  [the, recent, advancements, in, artificial, in...   \n",
              "2  [in, this, paper, we, proposed, a, novel, mutu...   \n",
              "3  [consistency, training, has, proven, to, be, a...   \n",
              "4  [to, ensure, safety, in, automated, driving, t...   \n",
              "\n",
              "                                  filtered_summaries  \\\n",
              "0  [stereo, matching, one, widely, used, techniqu...   \n",
              "1  [recent, advancements, artificial, intelligenc...   \n",
              "2  [paper, proposed, novel, mutual, consistency, ...   \n",
              "3  [consistency, training, proven, advanced, semi...   \n",
              "4  [ensure, safety, automated, driving, correct, ...   \n",
              "\n",
              "                                lemmatized_summaries  \\\n",
              "0  [stereo, matching, one, widely, used, techniqu...   \n",
              "1  [recent, advancement, artificial, intelligence...   \n",
              "2  [paper, proposed, novel, mutual, consistency, ...   \n",
              "3  [consistency, training, proven, advanced, semi...   \n",
              "4  [ensure, safety, automated, driving, correct, ...   \n",
              "\n",
              "                                     clean_summaries  \\\n",
              "0  stereo matching one widely used technique infe...   \n",
              "1  recent advancement artificial intelligence ai ...   \n",
              "2  paper proposed novel mutual consistency networ...   \n",
              "3  consistency training proven advanced semisuper...   \n",
              "4  ensure safety automated driving correct percep...   \n",
              "\n",
              "                            clean_summaries_pipeline  \\\n",
              "0  stereo matching one widely used technique infe...   \n",
              "1  recent advancement artificial intelligence ai ...   \n",
              "2  paper proposed novel mutual consistency networ...   \n",
              "3  consistency training proven advanced semisuper...   \n",
              "4  ensure safety automated driving correct percep...   \n",
              "\n",
              "                               clean_summaries_spacy  \n",
              "0  stereo matching widely technique infer depth s...  \n",
              "1  recent advancement artificial intelligence ai ...  \n",
              "2  paper propose novel mutual consistency network...  \n",
              "3  consistency training prove advanced semisuperv...  \n",
              "4  ensure safety automated drive correct percepti...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5209fb0-805c-467e-adfa-80332783795b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>titles</th>\n",
              "      <th>summaries</th>\n",
              "      <th>terms</th>\n",
              "      <th>processed_summaries</th>\n",
              "      <th>tokenized_summaries</th>\n",
              "      <th>filtered_summaries</th>\n",
              "      <th>lemmatized_summaries</th>\n",
              "      <th>clean_summaries</th>\n",
              "      <th>clean_summaries_pipeline</th>\n",
              "      <th>clean_summaries_spacy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
              "      <td>Stereo matching is one of the widely used tech...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "      <td>stereo matching is one of the widely used tech...</td>\n",
              "      <td>[stereo, matching, is, one, of, the, widely, u...</td>\n",
              "      <td>[stereo, matching, one, widely, used, techniqu...</td>\n",
              "      <td>[stereo, matching, one, widely, used, techniqu...</td>\n",
              "      <td>stereo matching one widely used technique infe...</td>\n",
              "      <td>stereo matching one widely used technique infe...</td>\n",
              "      <td>stereo matching widely technique infer depth s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
              "      <td>The recent advancements in artificial intellig...</td>\n",
              "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
              "      <td>the recent advancements in artificial intellig...</td>\n",
              "      <td>[the, recent, advancements, in, artificial, in...</td>\n",
              "      <td>[recent, advancements, artificial, intelligenc...</td>\n",
              "      <td>[recent, advancement, artificial, intelligence...</td>\n",
              "      <td>recent advancement artificial intelligence ai ...</td>\n",
              "      <td>recent advancement artificial intelligence ai ...</td>\n",
              "      <td>recent advancement artificial intelligence ai ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
              "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
              "      <td>['cs.CV', 'cs.AI']</td>\n",
              "      <td>in this paper we proposed a novel mutual consi...</td>\n",
              "      <td>[in, this, paper, we, proposed, a, novel, mutu...</td>\n",
              "      <td>[paper, proposed, novel, mutual, consistency, ...</td>\n",
              "      <td>[paper, proposed, novel, mutual, consistency, ...</td>\n",
              "      <td>paper proposed novel mutual consistency networ...</td>\n",
              "      <td>paper proposed novel mutual consistency networ...</td>\n",
              "      <td>paper propose novel mutual consistency network...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
              "      <td>Consistency training has proven to be an advan...</td>\n",
              "      <td>['cs.CV']</td>\n",
              "      <td>consistency training has proven to be an advan...</td>\n",
              "      <td>[consistency, training, has, proven, to, be, a...</td>\n",
              "      <td>[consistency, training, proven, advanced, semi...</td>\n",
              "      <td>[consistency, training, proven, advanced, semi...</td>\n",
              "      <td>consistency training proven advanced semisuper...</td>\n",
              "      <td>consistency training proven advanced semisuper...</td>\n",
              "      <td>consistency training prove advanced semisuperv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Background-Foreground Segmentation for Interio...</td>\n",
              "      <td>To ensure safety in automated driving, the cor...</td>\n",
              "      <td>['cs.CV', 'cs.LG']</td>\n",
              "      <td>to ensure safety in automated driving the corr...</td>\n",
              "      <td>[to, ensure, safety, in, automated, driving, t...</td>\n",
              "      <td>[ensure, safety, automated, driving, correct, ...</td>\n",
              "      <td>[ensure, safety, automated, driving, correct, ...</td>\n",
              "      <td>ensure safety automated driving correct percep...</td>\n",
              "      <td>ensure safety automated driving correct percep...</td>\n",
              "      <td>ensure safety automated drive correct percepti...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5209fb0-805c-467e-adfa-80332783795b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f5209fb0-805c-467e-adfa-80332783795b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f5209fb0-805c-467e-adfa-80332783795b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"titles\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"PI-RCNN: An Efficient Multi-sensor 3D Object Detector with Point-based Attentive Cont-conv Fusion Module\",\n          \"Studying the Plasticity in Deep Convolutional Neural Networks using Random Pruning\",\n          \"A Gentle Introduction to Deep Learning in Medical Image Processing\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"LIDAR point clouds and RGB-images are both extremely essential for 3D object\\ndetection. So many state-of-the-art 3D detection algorithms dedicate in fusing\\nthese two types of data effectively. However, their fusion methods based on\\nBirds Eye View (BEV) or voxel format are not accurate. In this paper, we\\npropose a novel fusion approach named Point-based Attentive Cont-conv\\nFusion(PACF) module, which fuses multi-sensor features directly on 3D points.\\nExcept for continuous convolution, we additionally add a Point-Pooling and an\\nAttentive Aggregation to make the fused features more expressive. Moreover,\\nbased on the PACF module, we propose a 3D multi-sensor multi-task network\\ncalled Pointcloud-Image RCNN(PI-RCNN as brief), which handles the image\\nsegmentation and 3D object detection tasks. PI-RCNN employs a segmentation\\nsub-network to extract full-resolution semantic feature maps from images and\\nthen fuses the multi-sensor features via powerful PACF module. Beneficial from\\nthe effectiveness of the PACF module and the expressive semantic features from\\nthe segmentation module, PI-RCNN can improve much in 3D object detection. We\\ndemonstrate the effectiveness of the PACF module and PI-RCNN on the KITTI 3D\\nDetection benchmark, and our method can achieve state-of-the-art on the metric\\nof 3D AP.\",\n          \"Recently there has been a lot of work on pruning filters from deep\\nconvolutional neural networks (CNNs) with the intention of reducing\\ncomputations.The key idea is to rank the filters based on a certain criterion\\n(say, l1-norm) and retain only the top ranked filters. Once the low scoring\\nfilters are pruned away the remainder of the network is fine tuned and is shown\\nto give performance comparable to the original unpruned network. In this work,\\nwe report experiments which suggest that the comparable performance of the\\npruned network is not due to the specific criterion chosen but due to the\\ninherent plasticity of deep neural networks which allows them to recover from\\nthe loss of pruned filters once the rest of the filters are fine-tuned.\\nSpecifically we show counter-intuitive results wherein by randomly pruning\\n25-50% filters from deep CNNs we are able to obtain the same performance as\\nobtained by using state-of-the-art pruning methods. We empirically validate our\\nclaims by doing an exhaustive evaluation with VGG-16 and ResNet-50. We also\\nevaluate a real world scenario where a CNN trained on all 1000 ImageNet classes\\nneeds to be tested on only a small set of classes at test time (say, only\\nanimals). We create a new benchmark dataset from ImageNet to evaluate such\\nclass specific pruning and show that even here a random pruning strategy gives\\nclose to state-of-the-art performance. Unlike existing approaches which mainly\\nfocus on the task of image classification, in this work we also report results\\non object detection and image segmentation. We show that using a simple random\\npruning strategy we can achieve significant speed up in object detection (74%\\nimprovement in fps) while retaining the same accuracy as that of the original\\nFaster RCNN model. Similarly we show that the performance of a pruned\\nSegmentation Network (SegNet) is actually very similar to that of the original\\nunpruned SegNet.\",\n          \"This paper tries to give a gentle introduction to deep learning in medical\\nimage processing, proceeding from theoretical foundations to applications. We\\nfirst discuss general reasons for the popularity of deep learning, including\\nseveral major breakthroughs in computer science. Next, we start reviewing the\\nfundamental basics of the perceptron and neural networks, along with some\\nfundamental theory that is often omitted. Doing so allows us to understand the\\nreasons for the rise of deep learning in many application domains. Obviously\\nmedical image processing is one of these areas which has been largely affected\\nby this rapid progress, in particular in image detection and recognition, image\\nsegmentation, image registration, and computer-aided diagnosis. There are also\\nrecent trends in physical simulation, modelling, and reconstruction that have\\nled to astonishing results. Yet, some of these approaches neglect prior\\nknowledge and hence bear the risk of producing implausible results. These\\napparent weaknesses highlight current limitations of deep learning. However, we\\nalso briefly discuss promising approaches that might be able to resolve these\\nproblems in the future.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"terms\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 133,\n        \"samples\": [\n          \"['cs.CV', 'eess.IV', 'stat.AP', '62P99']\",\n          \"['cs.CV', 'cs.LG', 'I.2.1, I.4.6,']\",\n          \"['cs.CV', 'cs.CR', 'cs.LG']\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"lidar point clouds and rgbimages are both extremely essential for 3d object detection so many stateoftheart 3d detection algorithms dedicate in fusing these two types of data effectively however their fusion methods based on birds eye view bev or voxel format are not accurate in this paper we propose a novel fusion approach named pointbased attentive contconv fusionpacf module which fuses multisensor features directly on 3d points except for continuous convolution we additionally add a pointpooling and an attentive aggregation to make the fused features more expressive moreover based on the pacf module we propose a 3d multisensor multitask network called pointcloudimage rcnnpircnn as brief which handles the image segmentation and 3d object detection tasks pircnn employs a segmentation subnetwork to extract fullresolution semantic feature maps from images and then fuses the multisensor features via powerful pacf module beneficial from the effectiveness of the pacf module and the expressive semantic features from the segmentation module pircnn can improve much in 3d object detection we demonstrate the effectiveness of the pacf module and pircnn on the kitti 3d detection benchmark and our method can achieve stateoftheart on the metric of 3d ap\",\n          \"recently there has been a lot of work on pruning filters from deep convolutional neural networks cnns with the intention of reducing computationsthe key idea is to rank the filters based on a certain criterion say l1norm and retain only the top ranked filters once the low scoring filters are pruned away the remainder of the network is fine tuned and is shown to give performance comparable to the original unpruned network in this work we report experiments which suggest that the comparable performance of the pruned network is not due to the specific criterion chosen but due to the inherent plasticity of deep neural networks which allows them to recover from the loss of pruned filters once the rest of the filters are finetuned specifically we show counterintuitive results wherein by randomly pruning 2550 filters from deep cnns we are able to obtain the same performance as obtained by using stateoftheart pruning methods we empirically validate our claims by doing an exhaustive evaluation with vgg16 and resnet50 we also evaluate a real world scenario where a cnn trained on all 1000 imagenet classes needs to be tested on only a small set of classes at test time say only animals we create a new benchmark dataset from imagenet to evaluate such class specific pruning and show that even here a random pruning strategy gives close to stateoftheart performance unlike existing approaches which mainly focus on the task of image classification in this work we also report results on object detection and image segmentation we show that using a simple random pruning strategy we can achieve significant speed up in object detection 74 improvement in fps while retaining the same accuracy as that of the original faster rcnn model similarly we show that the performance of a pruned segmentation network segnet is actually very similar to that of the original unpruned segnet\",\n          \"this paper tries to give a gentle introduction to deep learning in medical image processing proceeding from theoretical foundations to applications we first discuss general reasons for the popularity of deep learning including several major breakthroughs in computer science next we start reviewing the fundamental basics of the perceptron and neural networks along with some fundamental theory that is often omitted doing so allows us to understand the reasons for the rise of deep learning in many application domains obviously medical image processing is one of these areas which has been largely affected by this rapid progress in particular in image detection and recognition image segmentation image registration and computeraided diagnosis there are also recent trends in physical simulation modelling and reconstruction that have led to astonishing results yet some of these approaches neglect prior knowledge and hence bear the risk of producing implausible results these apparent weaknesses highlight current limitations of deep learning however we also briefly discuss promising approaches that might be able to resolve these problems in the future\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokenized_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"filtered_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lemmatized_summaries\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_summaries\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"lidar point cloud rgbimages extremely essential 3d object detection many stateoftheart 3d detection algorithm dedicate fusing two type data effectively however fusion method based bird eye view bev voxel format accurate paper propose novel fusion approach named pointbased attentive contconv fusionpacf module fuse multisensor feature directly 3d point except continuous convolution additionally add pointpooling attentive aggregation make fused feature expressive moreover based pacf module propose 3d multisensor multitask network called pointcloudimage rcnnpircnn brief handle image segmentation 3d object detection task pircnn employ segmentation subnetwork extract fullresolution semantic feature map image fuse multisensor feature via powerful pacf module beneficial effectiveness pacf module expressive semantic feature segmentation module pircnn improve much 3d object detection demonstrate effectiveness pacf module pircnn kitti 3d detection benchmark method achieve stateoftheart metric 3d ap\",\n          \"recently lot work pruning filter deep convolutional neural network cnns intention reducing computationsthe key idea rank filter based certain criterion say l1norm retain top ranked filter low scoring filter pruned away remainder network fine tuned shown give performance comparable original unpruned network work report experiment suggest comparable performance pruned network due specific criterion chosen due inherent plasticity deep neural network allows recover loss pruned filter rest filter finetuned specifically show counterintuitive result wherein randomly pruning 2550 filter deep cnns able obtain performance obtained using stateoftheart pruning method empirically validate claim exhaustive evaluation vgg16 resnet50 also evaluate real world scenario cnn trained 1000 imagenet class need tested small set class test time say animal create new benchmark dataset imagenet evaluate class specific pruning show even random pruning strategy give close stateoftheart performance unlike existing approach mainly focus task image classification work also report result object detection image segmentation show using simple random pruning strategy achieve significant speed object detection 74 improvement fps retaining accuracy original faster rcnn model similarly show performance pruned segmentation network segnet actually similar original unpruned segnet\",\n          \"paper try give gentle introduction deep learning medical image processing proceeding theoretical foundation application first discus general reason popularity deep learning including several major breakthrough computer science next start reviewing fundamental basic perceptron neural network along fundamental theory often omitted allows u understand reason rise deep learning many application domain obviously medical image processing one area largely affected rapid progress particular image detection recognition image segmentation image registration computeraided diagnosis also recent trend physical simulation modelling reconstruction led astonishing result yet approach neglect prior knowledge hence bear risk producing implausible result apparent weakness highlight current limitation deep learning however also briefly discus promising approach might able resolve problem future\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_summaries_pipeline\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"lidar point cloud rgbimages extremely essential 3d object detection many stateoftheart 3d detection algorithm dedicate fusing two type data effectively however fusion method based bird eye view bev voxel format accurate paper propose novel fusion approach named pointbased attentive contconv fusionpacf module fuse multisensor feature directly 3d point except continuous convolution additionally add pointpooling attentive aggregation make fused feature expressive moreover based pacf module propose 3d multisensor multitask network called pointcloudimage rcnnpircnn brief handle image segmentation 3d object detection task pircnn employ segmentation subnetwork extract fullresolution semantic feature map image fuse multisensor feature via powerful pacf module beneficial effectiveness pacf module expressive semantic feature segmentation module pircnn improve much 3d object detection demonstrate effectiveness pacf module pircnn kitti 3d detection benchmark method achieve stateoftheart metric 3d ap\",\n          \"recently lot work pruning filter deep convolutional neural network cnns intention reducing computationsthe key idea rank filter based certain criterion say l1norm retain top ranked filter low scoring filter pruned away remainder network fine tuned shown give performance comparable original unpruned network work report experiment suggest comparable performance pruned network due specific criterion chosen due inherent plasticity deep neural network allows recover loss pruned filter rest filter finetuned specifically show counterintuitive result wherein randomly pruning 2550 filter deep cnns able obtain performance obtained using stateoftheart pruning method empirically validate claim exhaustive evaluation vgg16 resnet50 also evaluate real world scenario cnn trained 1000 imagenet class need tested small set class test time say animal create new benchmark dataset imagenet evaluate class specific pruning show even random pruning strategy give close stateoftheart performance unlike existing approach mainly focus task image classification work also report result object detection image segmentation show using simple random pruning strategy achieve significant speed object detection 74 improvement fps retaining accuracy original faster rcnn model similarly show performance pruned segmentation network segnet actually similar original unpruned segnet\",\n          \"paper try give gentle introduction deep learning medical image processing proceeding theoretical foundation application first discus general reason popularity deep learning including several major breakthrough computer science next start reviewing fundamental basic perceptron neural network along fundamental theory often omitted allows u understand reason rise deep learning many application domain obviously medical image processing one area largely affected rapid progress particular image detection recognition image segmentation image registration computeraided diagnosis also recent trend physical simulation modelling reconstruction led astonishing result yet approach neglect prior knowledge hence bear risk producing implausible result apparent weakness highlight current limitation deep learning however also briefly discus promising approach might able resolve problem future\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_summaries_spacy\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"lidar point cloud rgbimage extremely essential 3d object detection stateoftheart 3d detection algorithm dedicate fuse type datum effectively fusion method base bird eye view bev voxel format accurate paper propose novel fusion approach name pointbase attentive contconv fusionpacf module fuse multisensor feature directly 3d point continuous convolution additionally add pointpooling attentive aggregation fuse feature expressive base pacf module propose 3d multisensor multitask network call pointcloudimage rcnnpircnn brief handle image segmentation 3d object detection task pircnn employ segmentation subnetwork extract fullresolution semantic feature map image fuse multisensor feature powerful pacf module beneficial effectiveness pacf module expressive semantic feature segmentation module pircnn improve 3d object detection demonstrate effectiveness pacf module pircnn kitti 3d detection benchmark method achieve stateoftheart metric 3d ap\",\n          \"recently lot work prune filter deep convolutional neural network cnn intention reduce computationsthe key idea rank filter base certain criterion l1norm retain rank filter low scoring filter prune away remainder network fine tune show performance comparable original unpruned network work report experiment suggest comparable performance pruned network specific criterion choose inherent plasticity deep neural network allow recover loss prune filter rest filter finetune specifically counterintuitive result randomly prune 2550 filter deep cnn able obtain performance obtain stateoftheart pruning method empirically validate claim exhaustive evaluation vgg16 resnet50 evaluate real world scenario cnn train 1000 imagenet class need test small set class test time animal create new benchmark dataset imagenet evaluate class specific pruning random pruning strategy give close stateoftheart performance unlike exist approach mainly focus task image classification work report result object detection image segmentation simple random pruning strategy achieve significant speed object detection 74 improvement fps retain accuracy original fast rcnn model similarly performance pruned segmentation network segnet actually similar original unprune segnet\",\n          \"paper try gentle introduction deep learning medical image processing proceeding theoretical foundation application discuss general reason popularity deep learning include major breakthrough computer science start review fundamental basic perceptron neural network fundamental theory omit allow understand reason rise deep learning application domain obviously medical image processing area largely affect rapid progress particular image detection recognition image segmentation image registration computeraide diagnosis recent trend physical simulation modelling reconstruction lead astonishing result approach neglect prior knowledge bear risk produce implausible result apparent weakness highlight current limitation deep learning briefly discuss promise approach able resolve problem future\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    }
  ]
}